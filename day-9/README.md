# README

This CUDA program applies the GELU (Gaussian Error Linear Unit) activation function to a vector of 32-bit floating point numbers. The computation is performed in parallel on the GPU. The program demonstrates how to implement and use the GELU activation function efficiently with CUDA for batch data processing. Input and output values are printed for verification.